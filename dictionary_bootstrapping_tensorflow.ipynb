{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import _pickle as pk\n",
    "import numpy as np\n",
    "# from src.helper_funcs import save_model, visualize_embeddings\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set directories and basic model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base path\n",
    "base_path = 'C:/Users/admin/PycharmProjects'\n",
    "\n",
    "# Path to input data.\n",
    "data_in = os.path.join(base_path,'Byungkon_data/simple-data.pkl')\n",
    "with open(data_in, 'rb') as f:\n",
    "    d = pk.load(f, encoding='latin1')\n",
    "\n",
    "# Path to tensorboard output files\n",
    "log_dir = os.path.join(base_path,'tensorboard_output')\n",
    "\n",
    "# model parameters path\n",
    "models_dir = os.path.join(base_path,'models')\n",
    "\n",
    "bs = 64  # mini-batch size\n",
    "td = 300 # embedding dimension\n",
    "hd = 600  # hidden dimension\n",
    "\n",
    "nw, mw, ms = d['def'].shape  # total number of words, max num. of words per definition, max num. of senses per word\n",
    "# nw = len(d['id2dw'])\n",
    "params = {}                  # hash to hold the trainable parameters\n",
    "tau = 10\n",
    "# We need the following 3 lines to account for the discrepancy between\n",
    "# the true number of words (true_nw) vs. the number of words w/ IDs (nw).\n",
    "# This discrepancy exists because of some designs choices that have not been modified.\n",
    "true_nw = nw\n",
    "maxid = np.max(d['def'])\n",
    "if maxid >= nw:\n",
    "    nw = maxid + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define parameters. (Placeholders, trainable 'n' non-trainable params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Input placeholders and constant values\n",
    "\"\"\"\n",
    "# placeholders\n",
    "df = tf.placeholder(name='def', dtype=tf.int32, shape=[None, mw, ms])  # Takes values from d['def']\n",
    "dm = tf.placeholder(name='dmask', dtype=tf.float32, shape=[None, mw, ms])  # Takes values from d['dmask']\n",
    "wmask = tf.placeholder(name='wmask', dtype=tf.float32, shape=[None, mw])  # Takes values from d['wmask']\n",
    "h_d = tf.placeholder(name='idf', dtype=tf.float32, shape=[None, mw]) # Takes values from d['idf']\n",
    "wi = tf.placeholder(name='wi', dtype=tf.int32)    # batch of word indices\n",
    "# nwi = tf.placeholder(name='nwi', dtype=tf.int32)  # batch of word indices (negative samples- if Hinge Loss is used)\n",
    "pr = tf.placeholder(name='sprior', dtype=tf.float32, shape=[None, mw, ms])\n",
    "lr = tf.placeholder(name='lr', dtype=tf.float64)  # learning rate\n",
    "beta = tf.placeholder(name='beta', dtype=tf.float32) # beta for fixed point iteration\n",
    "\n",
    "# constants\n",
    "# NO constants for now\n",
    "\n",
    "\"\"\"\n",
    "    Non-trainable parameters\n",
    "\"\"\"\n",
    "params['dwe'] = tf.get_variable(name='dwe',\n",
    "                                shape=(nw, td),\n",
    "                                dtype=tf.float32,\n",
    "                                initializer=tf.initializers.random_uniform(minval=-0.1, maxval=+0.1),\n",
    "                                trainable=False)  # disambiguated word embedding shape=(nw, td)\n",
    "\n",
    "\"\"\"\n",
    "    Trainable parameters\n",
    "\"\"\"\n",
    "params['L'] = tf.get_variable('L', shape=(td, ), dtype=tf.float32, initializer=tf.initializers.random_uniform(minval=-0.1, maxval=+0.1))  # td: diagonal entries only (of the td x td matrix)\n",
    "params['L1'] = tf.get_variable('L1', shape=(td, hd), dtype=tf.float32, initializer=tf.initializers.random_uniform(minval=-0.1, maxval=+0.1))  # td x hd\n",
    "params['L2'] = tf.get_variable('L2', shape=(hd, td), dtype=tf.float32, initializer=tf.initializers.random_uniform(minval=-0.1, maxval=+0.1))  # hd x td\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gathering --> selecting embeddings, Expanding --> for matrix multiplication below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Word vectors gathering\n",
    "\"\"\"\n",
    "with tf.name_scope('Gather_WEs'):\n",
    "    # ndw = tf.gather(params['dwe'], nwi, name='neg_samp')  # bs x td (negative samples)\n",
    "    pdw = tf.gather(params['dwe'], wi, name='pos_samp')  # bs x td  (positive samples)\n",
    "\n",
    "\"\"\"\n",
    "    Expanding tensors to help with operations below  \n",
    "\"\"\"\n",
    "with tf.name_scope('Expand_dims'):\n",
    "    wm = tf.expand_dims(wmask, axis=2)  # bs x mw x 1\n",
    "    idf = tf.expand_dims(h_d, axis=2)  # bs x mw x 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Return alpha coefficients\n",
    "\"\"\"\n",
    "with tf.name_scope('Alpha_coeffs'):\n",
    "\n",
    "    def to_weight(_, args_):\n",
    "        \"\"\"\n",
    "            Returns: alpha coefficients of size mw x ms.\n",
    "            d: indices of the senses that comprise the definition (mw x ms)\n",
    "            m: mask of size mw x ms\n",
    "            prior: Pr(v_j) for each v_j in each plain word in the definition (mw x ms)\n",
    "        \"\"\"\n",
    "        d, m, prior = args_\n",
    "\n",
    "        half_dot = tf.gather(params['dwe'], d) * tf.reshape(params['L'], [1, 1, td], name='half_dot')  # mw x ms x td\n",
    "        logit = tf.tensordot(half_dot, tf.transpose(half_dot), axes=1, name='U_pos_def_matrix')  # mw x ms x ms x mw\n",
    "        logit = tf.transpose(logit, perm=[0, 1, 3, 2], name='shuffle_U_inds')  # mw x ms x mw x ms\n",
    "        cnt = tf.reshape(tf.reduce_sum(m, axis=1), [1, 1, mw], name='cnt')  # 1 x 1 x mw (In other words, |s(d_m)|)\n",
    "        logit = tf.reduce_sum(logit * tf.reshape(m, [1, 1, mw, ms]), axis=3, name='logit_3') / cnt # mw x ms x mw\n",
    "        logit = tf.exp(tau * tf.where(tf.is_nan(logit), tf.zeros_like(logit), logit), name='logit_4') # mw x ms x mw\n",
    "        logit = tf.reduce_prod(logit, axis=2, name='logit_5') * prior # mw x ms\n",
    "        sm = tf.reduce_sum(logit * m, axis=1, keepdims=True, name='sm') # mw x 1\n",
    "        logit = (logit* m) / sm # mw x ms\n",
    "        res = tf.where(tf.logical_or(tf.is_nan(logit), tf.is_inf(logit)), tf.zeros_like(logit), logit, name='to_weight_res')\n",
    "\n",
    "        return res\n",
    "\n",
    "\n",
    "    alphas = tf.scan(to_weight, [df, dm, pr], initializer=tf.zeros(shape=[mw, ms]), name='alphas')  # bs x mw x ms\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-Layer NN for regression training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Calculate the double convex combination of senses of all plain words for a given definition and pass the resulting\n",
    "    output to a 2-layer NN to get the new embedding\n",
    "\"\"\"\n",
    "with tf.name_scope('Convex_comb_senses'):\n",
    "\n",
    "    raw_emb = tf.reduce_sum(tf.expand_dims(alphas, axis=3) * tf.gather(params['dwe'], df), axis=2, name='senses_sum')  # bs x mw x td\n",
    "    e_i = tf.reduce_sum((raw_emb * idf) * wm, axis=1, name='pl_words_sum')  # bs x td\n",
    "\n",
    "with tf.name_scope(\"2_Layer_NN\"):\n",
    "\n",
    "    new_emb = tf.tanh(tf.matmul(e_i, params['L1'], name='L1'))  # bs x hd\n",
    "    new_emb = tf.matmul(new_emb, params['L2'], name='L2')  # bs x td, after passing through a 2-layer network\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss - Optimization ( Fixed point iteration & regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name L:0/weights is illegal; using L_0/weights instead.\n",
      "INFO:tensorflow:Summary name L:0/gradient is illegal; using L_0/gradient instead.\n",
      "INFO:tensorflow:Summary name L1:0/weights is illegal; using L1_0/weights instead.\n",
      "INFO:tensorflow:Summary name L1:0/gradient is illegal; using L1_0/gradient instead.\n",
      "INFO:tensorflow:Summary name L2:0/weights is illegal; using L2_0/weights instead.\n",
      "INFO:tensorflow:Summary name L2:0/gradient is illegal; using L2_0/gradient instead.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Loss for regression\n",
    "\"\"\"\n",
    "with tf.name_scope(\"Loss_calc\"):\n",
    "    # Uncomment the following for NORMALIZATION\n",
    "    # new_emb_norm = tf.nn.l2_normalize(new_emb, axis=1)\n",
    "    # pdw_norm = tf.nn.l2_normalize(pdw, axis=1)\n",
    "\n",
    "    # Uncomment the following for REGULARIZATION\n",
    "    # l1_l2_reg = tf.contrib.layers.l1_l2_regularizer(scale_l1=0.0001, scale_l2=0.0001)\n",
    "    # reg_penalty = tf.contrib.layers.apply_regularization(l1_l2_reg, [params['L1'], params['L2']])\n",
    "\n",
    "    # Choose loss function\n",
    "    # loss = tf.reduce_mean(tf.abs(pdw_norm - new_emb_norm))\n",
    "    loss = tf.losses.mean_squared_error(pdw, new_emb)\n",
    "\n",
    "\"\"\"\n",
    "    Optimization\n",
    "\"\"\"\n",
    "with tf.name_scope('optimization_via_grads'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "    gradients, variables = zip(*optimizer.compute_gradients(loss))\n",
    "\n",
    "    # clip a small value to deal with vanishing and exploding gradients\n",
    "    # if i don't add these lines, i get NaN values in gradients\n",
    "    gradients = [\n",
    "        None if gradient is None else tf.where(tf.logical_or(tf.is_nan(gradient), tf.is_inf(gradient)), tf.zeros_like(gradient), gradient)\n",
    "        for gradient in gradients]\n",
    "    gradients_, _ = tf.clip_by_global_norm(gradients, 1e-10)\n",
    "\n",
    "    # Summarize all gradients and weights\n",
    "    for grad, var in zip(gradients_, variables):\n",
    "        tf.summary.histogram(var.name + '/weights', var)\n",
    "        tf.summary.histogram(var.name + '/gradient', grad)\n",
    "    train_op = optimizer.apply_gradients(zip(gradients_, variables))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Fixed-point update\n",
    "\"\"\"\n",
    "with tf.name_scope(\"fixed_point_update\"):\n",
    "    fp_emb = (1 - beta) * pdw + beta * e_i\n",
    "    fp_update = tf.scatter_update(params['dwe'], wi, fp_emb)  # we only update a portion of the embeddings\n",
    "    dwe_diff = tf.reduce_max(tf.abs(fp_emb - pdw))  # maximum increment\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 82831 data....\n",
      "################################################################################\n",
      "Now processing data for epoch: 0\n",
      "################################################################################\n",
      "Current mode:fp\n",
      "Accumulated loss (1920 of 82831 data): 0.11695722211152315\n",
      "Current mode:fp\n",
      "Accumulated loss (3840 of 82831 data): 0.23341099615208805\n",
      "Current mode:fp\n",
      "Accumulated loss (5760 of 82831 data): 0.34956910577602684\n",
      "Current mode:fp\n",
      "Accumulated loss (7680 of 82831 data): 0.46582138258963823\n",
      "Current mode:fp\n",
      "Accumulated loss (9600 of 82831 data): 0.5813017392065376\n",
      "Current mode:fp\n",
      "Accumulated loss (11520 of 82831 data): 0.6970490997191519\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-673fc48d0b39>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     95\u001b[0m                 feed_d = {wi: wis, df: d['def'][wis], dm: d['dmask'][wis], wmask: d['wmask'][wis], h_d: d['idf'][wis],\n\u001b[0;32m     96\u001b[0m                           beta: beta_val, pr: priors}\n\u001b[1;32m---> 97\u001b[1;33m                 \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmerged_summary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfp_update\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdwe_diff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmerged_summary_op\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_d\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m                 \u001b[1;31m# record to Tensorboard\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    885\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 887\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    888\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1108\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1110\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1111\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1284\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1286\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1287\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1288\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1290\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1291\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1292\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1293\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1294\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1275\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1277\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1279\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1365\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1366\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1367\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1368\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1369\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    TRAINING PROCESS\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init_all_op = tf.global_variables_initializer()\n",
    "    sess.run(init_all_op)\n",
    "\n",
    "    # Merge all summaries into a single op\n",
    "    merged_summary_op = tf.summary.merge_all()\n",
    "    # tensorboard line for the Graph\n",
    "    summary_writer = tf.summary.FileWriter(log_dir, sess.graph)\n",
    "    # python variable summary (Mean batch loss value)\n",
    "    m_b_loss_summ = tf.Summary()\n",
    "\n",
    "    print('Training on ' + str(true_nw) + \" data....\")\n",
    "\n",
    "    # create the \"pool\" of indices for batch creation\n",
    "    indices_pool = np.array([i for i in range(true_nw)])\n",
    "\n",
    "    # basic training parameters\n",
    "    num_epoch = 10  # total number of epochs to train\n",
    "    cur_ep = 0\n",
    "    cur_lr = 5e-2\n",
    "    num_consec_train = 5  # number of consecutive epochs for SGD\n",
    "    mode = ['sgd', 'fp']\n",
    "    cur_mode = 1  # start with 'sgd'. Set this to 1 if you want to start with fp\n",
    "    tol = 1e-3\n",
    "    next_schedule = 4  # ( num_consec_train - 1)\n",
    "    dwe_up_cnt = 0\n",
    "    tic = 30\n",
    "    beta_val = 0.8\n",
    "    # entered_fp = 1\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "\n",
    "        # Print epoch\n",
    "        print(\"################################################################################\")\n",
    "        print(\"Now processing data for epoch: \" + str(epoch))\n",
    "        print(\"################################################################################\")\n",
    "\n",
    "        # counter for processed data ( each epoch )\n",
    "        cnt_processed = 0\n",
    "\n",
    "        # total loss\n",
    "        epoch_loss = 0\n",
    "\n",
    "        # shuffle before slicing\n",
    "        np.random.shuffle(indices_pool)\n",
    "\n",
    "        cost = 0\n",
    "        totTime = 0\n",
    "        max_diff = -np.inf\n",
    "        cur_beta = beta_val ** (dwe_up_cnt + 1)\n",
    "\n",
    "        for i_s in range(0, true_nw, bs):\n",
    "\n",
    "            # i_s and i_e are the Starting and Ending indices of the indices_pool, and their are used to sample our\n",
    "            # shuffled dataset\n",
    "            i_e = i_s + bs if i_s + bs < true_nw else true_nw - 1\n",
    "\n",
    "            # word indices to be trained\n",
    "            wis = indices_pool[i_s: i_e]\n",
    "            # indices of the negative sample words\n",
    "            # cnt = 0\n",
    "            # nwis = []\n",
    "            # while cnt < len(wis):\n",
    "            #     rand_num = np.random.randint(0, true_nw)\n",
    "            #     if rand_num not in wis:\n",
    "            #         nwis.append(rand_num)\n",
    "            #         cnt += 1\n",
    "            # nwis = np.array(nwis)\n",
    "\n",
    "            # provide the priors\n",
    "            priors = np.ones(shape=(len(wis), mw, ms))\n",
    "\n",
    "            # increase the count of processed data\n",
    "            cnt_processed += len(wis)\n",
    "\n",
    "            # initialize batch_loss\n",
    "            batch_loss = 0\n",
    "\n",
    "            if mode[cur_mode] == 'sgd':\n",
    "\n",
    "                feed_d = {wi: wis, df: d['def'][wis], dm: d['dmask'][wis], wmask: d['wmask'][wis], h_d: d['idf'][wis],\n",
    "                          lr: cur_lr, pr: priors}\n",
    "                _, batch_loss, merged_summary = sess.run([train_op, loss, merged_summary_op], feed_dict=feed_d)\n",
    "\n",
    "                # record to Tensorboard\n",
    "                summary_writer.add_summary(merged_summary, epoch)\n",
    "\n",
    "            elif mode[cur_mode] == 'fp':\n",
    "\n",
    "                feed_d = {wi: wis, df: d['def'][wis], dm: d['dmask'][wis], wmask: d['wmask'][wis], h_d: d['idf'][wis],\n",
    "                          beta: beta_val, pr: priors}\n",
    "                _, batch_loss, diff, merged_summary = sess.run([fp_update, loss, dwe_diff, merged_summary_op], feed_dict=feed_d)\n",
    "\n",
    "                # record to Tensorboard\n",
    "                summary_writer.add_summary(merged_summary, epoch)\n",
    "\n",
    "                # Difference on update / updates counted\n",
    "                max_diff = max(max_diff, float(diff))\n",
    "                dwe_up_cnt += 1\n",
    "\n",
    "            epoch_loss += batch_loss\n",
    "            # print mini-batch loss every \"tic\" time\n",
    "            if (i_e // bs) % tic == 0:\n",
    "                print(\"Current mode:\" + mode[cur_mode])\n",
    "                print(\n",
    "                    \"Accumulated loss (\" + str(cnt_processed) + \" of \" + str(true_nw) + \" data): \" + str(epoch_loss))\n",
    "\n",
    "        # End of epoch --> mean batch loss\n",
    "        mean_batch_loss = epoch_loss / np.ceil((true_nw / bs))\n",
    "        m_b_loss_summ.value.add(tag='Mean_batch_loss', simple_value= mean_batch_loss)\n",
    "        summary_writer.add_summary(m_b_loss_summ, epoch)\n",
    "\n",
    "        # At the end of each epoch determine the transition of the training process\n",
    "        if cur_mode == 1 and max_diff < tol:\n",
    "            cur_mode = 0  # switch to SGD\n",
    "            max_diff = -np.inf\n",
    "            next_schedule = epoch + num_consec_train\n",
    "        elif cur_mode == 0 and next_schedule == epoch:\n",
    "            cur_mode = 1  # switch to fixed-point iteration\n",
    "            dwe_up_cnt = 0\n",
    "\n",
    "        # Here you can add a function for saving the model for each epoch\n",
    "        # TO-DO\n",
    "\n",
    "    # configure the projector\n",
    "    embeddings_writer = tf.summary.FileWriter(log_dir, sess.graph)\n",
    "    config = projector.ProjectorConfig()\n",
    "    config.model_checkpoint_path = os.path.join(models_dir, 'test_model.ckpt')\n",
    "    embedding_conf = config.embeddings.add()\n",
    "    embedding_conf.tensor_name = params['dwe'].name\n",
    "    embedding_conf.metadata_path = os.path.join(log_dir, 'metadata.tsv')\n",
    "    projector.visualize_embeddings(embeddings_writer, config)\n",
    "    # Exit training process, save model\n",
    "    save_model(sess, models_dir, 'test_model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
